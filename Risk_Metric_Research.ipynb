{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Numerai data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Numerai data is divided into 3 subsets:\n",
    " - training data\n",
    " - validation data\n",
    " - live data\n",
    " \n",
    "For training and validation data sets the target is known. The target information for live data is not provided. No one knows it (it is from the future). The goal of the competition is to predict target values for live data. Users predictions are used by Numerai to compute Meta Model (ensemble of all models from the competition). Numerai use Meta Model to submit positions to the market. Based on accuracy of user's prediction the rewards (payouts) are distributed to the participants in the Numerai token (Numeraire, ticker NMR). The reward depends on quality of predictions and stake value. The sake value is the amount of NMR the user is willing to bet on the predictions. Every week a new portion of live data is released. The users compute their predictions for live data and submit them to the Numerai service. The true target values are known for live data after 4 weeks from release and rewards are computed.  \n",
    "\n",
    "The reward can be computed in two ways (user decide which will be used):\n",
    "- reward based on correlation between predictions and targets on live data\n",
    "- reward based on correlation between predictions and targets on live data and MMC contribution \n",
    "\n",
    "Three things that needs to be clarified:\n",
    "- rewards are clipped on both side. The absolute correlation (or corr + MMC) value is clipped at 0.25.\n",
    "- reward can be nagative if the correlation (or corr+MMC) is negative, then your stake will be decreased corresponding to the clipped correlation (or corr+MMC) value\n",
    "- what is the MMC? The MMC is abbreviation for Meta Model Contribution. It is a metric that defines how much your predictions are important to the Numerai Meta Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it the hardest hardest data science tournament on the planet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of possible solutions is **endless**. The goal of the user is to select such solution that will be the most profitable in the long run. It is hard because the signal to noise ratio is very small.\n",
    "\n",
    "### How to do this?\n",
    "You need to check as many as possible different solutions and select the best one (of few the best, because in the tournament you can stake on up to 10 models). In this tutorial I will show you how to create a set of solutions. Then select the models that will **minimize the risk** and **maximize the profit**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, uniform\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Loading data...\n",
      "Loaded 310 features\n"
     ]
    }
   ],
   "source": [
    "print(\"# Loading data...\")\n",
    "# The training data is used to train your model how to predict the targets.\n",
    "training_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_training_data.csv.xz\").set_index(\"id\", drop=True).astype(\"float16\", errors=\"ignore\")\n",
    "# The tournament data is the data that Numerai uses to evaluate your model.\n",
    "tournament_data = pd.read_csv(\"https://numerai-public-datasets.s3-us-west-2.amazonaws.com/latest_numerai_tournament_data.csv.xz\").set_index(\"id\", drop=True).astype(\"float16\", errors=\"ignore\")\n",
    "validation_data = tournament_data[tournament_data[\"data_type\"] == \"validation\"]\n",
    "del tournament_data\n",
    "tournament_data = None\n",
    "gc.collect()\n",
    "\n",
    "feature_names = [f for f in training_data.columns if f.startswith(\"feature\")]\n",
    "print(f\"Loaded {len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and collect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data to 2 fold CV\n",
    "era_list = training_data[\"era\"].unique().tolist()\n",
    "h1_eras = era_list[:len(era_list)//2]\n",
    "h2_eras = era_list[len(era_list)//2:]\n",
    "\n",
    "h1_data = training_data[training_data[\"era\"].isin(h1_eras)]\n",
    "h2_data = training_data[training_data[\"era\"].isin(h2_eras)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a random search in the space of available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Predictions\n",
      "Xgboost_0, max_depth=5, learning_rate=0.01, n_estimators=2000, colsample_bytree=0.1, eras count=120, features count=310\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1776)\n",
    "\n",
    "predictions = []\n",
    "for i in range(300):\n",
    "    name = f\"Xgboost_{i}\"\n",
    "    max_depth = np.random.choice([2, 3, 4, 5])\n",
    "    learning_rate = np.random.choice([0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "    n_estimators = np.random.choice([10, 50, 100, 200])\n",
    "    colsample_bytree = np.random.choice([0.1, 0.2, 0.4, 0.7, 1.0])\n",
    "\n",
    "    features = feature_names # default, use all features\n",
    "    select_eras = era_list # default, use all eras\n",
    "    if i == 0:\n",
    "        print(\"Example Predictions\")\n",
    "        max_depth = 5\n",
    "        learning_rate = 0.01\n",
    "        n_estimators = 2000\n",
    "        colsample_bytree = 0.1\n",
    "    \n",
    "    if i >= 100:\n",
    "        print(\"Subsample columns\")\n",
    "        cols_count = np.random.randint(30, 270) # arbitrary numbers\n",
    "        features = np.random.choice(feature_names, cols_count, replace=False)\n",
    "        \n",
    "    if i >= 200:\n",
    "        print(\"Subsample eras\")\n",
    "        eras_count = np.random.randint(40, 110) # arbitrary numbers\n",
    "        select_eras = np.random.choice(era_list, eras_count, replace=False)\n",
    "        \n",
    "    print(f\"{name}, max_depth={max_depth}, learning_rate={learning_rate}, n_estimators={n_estimators}, colsample_bytree={colsample_bytree}, eras count={len(select_eras)}, features count={len(features)}\")\n",
    "\n",
    "    model = XGBRegressor(max_depth=max_depth,\n",
    "                         learning_rate=learning_rate,\n",
    "                         n_estimators=n_estimators,\n",
    "                         colsample_bytree=colsample_bytree,\n",
    "                         n_jobs=-1,\n",
    "                         verbosity=1,\n",
    "                         random_state=12)\n",
    "\n",
    "    # train on fold 1\n",
    "    model.fit(h1_data[features][h1_data[\"era\"].isin(select_eras)], h1_data[\"target_kazutsugi\"][h1_data[\"era\"].isin(select_eras)])\n",
    "    h2_preds = pd.Series(model.predict(h2_data[features]), index=h2_data.index)\n",
    "    validation_preds_fold_1 = pd.Series(model.predict(validation_data[features]), index=validation_data.index)\n",
    "    \n",
    "    \n",
    "\n",
    "    # train on fold 2\n",
    "    model.fit(h2_data[features][h2_data[\"era\"].isin(select_eras)], h2_data[\"target_kazutsugi\"][h2_data[\"era\"].isin(select_eras)])\n",
    "    h1_preds = pd.Series(model.predict(h1_data[features]), index=h1_data.index)\n",
    "    validation_preds_fold_2 = pd.Series(model.predict(validation_data[features]), index=validation_data.index)\n",
    "\n",
    "    # train on all training data\n",
    "    model.fit(training_data[features][training_data[\"era\"].isin(select_eras)], training_data[\"target_kazutsugi\"][training_data[\"era\"].isin(select_eras)])\n",
    "    validation_preds = pd.Series(model.predict(validation_data[features]), index=validation_data.index)\n",
    "\n",
    "    predictions += [{\n",
    "        \"name\": name,\n",
    "        \"training_preds\": pd.concat([h1_preds, h2_preds]),\n",
    "        \"validation_preds_cv\": (validation_preds_fold_1 + validation_preds_fold_2) / 2.0,\n",
    "        \"validation_preds\": validation_preds,\n",
    "    }]\n",
    "    pickle.dump(predictions, open(\"backup.pickle\", \"bw\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare predictions Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds_df = pd.DataFrame()\n",
    "validation_preds_cv_df = pd.DataFrame()\n",
    "validation_preds_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in predictions:\n",
    "    training_preds_df[pred[\"name\"]] = pred[\"training_preds\"]\n",
    "    validation_preds_cv_df[pred[\"name\"]] = pred[\"validation_preds_cv\"]\n",
    "    validation_preds_df[pred[\"name\"]] = pred[\"validation_preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds_df[\"era\"] = training_data[\"era\"]\n",
    "validation_preds_cv_df[\"era\"] = validation_data[\"era\"]\n",
    "validation_preds_df[\"era\"] = validation_data[\"era\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds_df[\"target\"] = training_data[\"target_kazutsugi\"]\n",
    "validation_preds_cv_df[\"target\"] = validation_data[\"target_kazutsugi\"]\n",
    "validation_preds_df[\"target\"] = validation_data[\"target_kazutsugi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them\n",
    "training_preds_df.to_csv(\"training_preds.csv\")\n",
    "validation_preds_cv_df.to_csv(\"validation_preds_cv.csv\")\n",
    "validation_preds_df.to_csv(\"validation_preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Scores per Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [c for c in validation_preds_df if \"era\" != c and \"target\" != c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores_per_era = validation_preds_df.groupby(\"era\").apply(lambda d: d[model_names].corrwith(d[\"target\"], method=\"spearman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_scores_per_era = training_preds_df.groupby(\"era\").apply(lambda d: d[model_names].corrwith(d[\"target\"], method=\"spearman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(index=model_names, columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean\n",
    "metrics_df[\"val_mean\"] = val_scores_per_era.mean()\n",
    "metrics_df[\"training_mean\"] = training_scores_per_era.mean()\n",
    "# Calcualte sharpe\n",
    "metrics_df[\"val_sharpe\"] = val_scores_per_era.mean()/val_scores_per_era.std()\n",
    "metrics_df[\"training_sharpe\"] = training_scores_per_era.mean()/training_scores_per_era.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Neutrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work in progress ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
